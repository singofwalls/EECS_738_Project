{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and Setup Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from globus_setup import *\n",
    "\n",
    "\n",
    "print(\"reading data\")\n",
    "Xpred = []\n",
    "X = []\n",
    "y = []\n",
    "with open(CSV_NAME, \"r\") as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "\n",
    "    training_fields = set(csv_reader.fieldnames) - {\"day\", \"tas\"}\n",
    "    for row in csv_reader:\n",
    "        # Remove rows which are missing values for any variable\n",
    "        empty_fields = [f for f in row if row[f] == \"\"]\n",
    "        if empty_fields:\n",
    "            continue\n",
    "        features = list(v for k, v in row.items() if k in training_fields)\n",
    "        if int(row[\"day\"]) == 42002:\n",
    "            Xpred.append(features)\n",
    "        else:\n",
    "            X.append(features)\n",
    "            y.append(row[\"tas\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.88690271\n",
      "Iteration 2, loss = 9.87022850\n",
      "Iteration 3, loss = 9.83529814\n",
      "Iteration 4, loss = 9.49661091\n",
      "Iteration 5, loss = 9.05806882\n",
      "Iteration 6, loss = 8.74422955\n",
      "Iteration 7, loss = 8.44576899\n",
      "Iteration 8, loss = 8.13116543\n",
      "Iteration 9, loss = 7.79554441\n",
      "Iteration 10, loss = 7.47130262\n",
      "Iteration 11, loss = 7.19540629\n",
      "Iteration 12, loss = 6.97186583\n",
      "Iteration 13, loss = 6.78893357\n",
      "Iteration 14, loss = 6.63563666\n",
      "Iteration 15, loss = 6.50307892\n",
      "Iteration 16, loss = 6.38624737\n",
      "Iteration 17, loss = 6.28290727\n",
      "Iteration 18, loss = 6.18841634\n",
      "Iteration 19, loss = 6.10169710\n",
      "Iteration 20, loss = 6.02410050\n",
      "Iteration 21, loss = 5.94811982\n",
      "Iteration 22, loss = 5.88137294\n",
      "Iteration 23, loss = 5.82047756\n",
      "Iteration 24, loss = 5.76130896\n",
      "Iteration 25, loss = 5.70438499\n",
      "Iteration 26, loss = 5.65236261\n",
      "Iteration 27, loss = 5.60220123\n",
      "Iteration 28, loss = 5.55650185\n",
      "Iteration 29, loss = 5.51119236\n",
      "Iteration 30, loss = 5.46902696\n",
      "Iteration 31, loss = 5.43128497\n",
      "Iteration 32, loss = 5.39105988\n",
      "Iteration 33, loss = 5.35167998\n",
      "Iteration 34, loss = 5.31709875\n",
      "Iteration 35, loss = 5.28215951\n",
      "Iteration 36, loss = 5.24774724\n",
      "Iteration 37, loss = 5.21794330\n",
      "Iteration 38, loss = 5.18547125\n",
      "Iteration 39, loss = 5.15804597\n",
      "Iteration 40, loss = 5.12609556\n",
      "Iteration 41, loss = 5.09937630\n",
      "Iteration 42, loss = 5.07162539\n",
      "Iteration 43, loss = 5.04637390\n",
      "Iteration 44, loss = 5.02087603\n",
      "Iteration 45, loss = 4.99820654\n",
      "Iteration 46, loss = 4.97420327\n",
      "Iteration 47, loss = 4.94728237\n",
      "Iteration 48, loss = 4.92670254\n",
      "Iteration 49, loss = 4.90386128\n",
      "Iteration 50, loss = 4.88191553\n",
      "Iteration 51, loss = 4.85972757\n",
      "Iteration 52, loss = 4.84298660\n",
      "Iteration 53, loss = 4.81937238\n",
      "Iteration 54, loss = 4.80057115\n",
      "Iteration 55, loss = 4.78101481\n",
      "Iteration 56, loss = 4.76287559\n",
      "Iteration 57, loss = 4.74795755\n",
      "Iteration 58, loss = 4.72673539\n",
      "Iteration 59, loss = 4.70690015\n",
      "Iteration 60, loss = 4.69191464\n",
      "Iteration 61, loss = 4.67491721\n",
      "Iteration 62, loss = 4.65620269\n",
      "Iteration 63, loss = 4.64234482\n",
      "Iteration 64, loss = 4.62393328\n",
      "Iteration 65, loss = 4.60900683\n",
      "Iteration 66, loss = 4.59633331\n",
      "Iteration 67, loss = 4.58053270\n",
      "Iteration 68, loss = 4.56754937\n",
      "Iteration 69, loss = 4.55091814\n",
      "Iteration 70, loss = 4.53838263\n",
      "Iteration 71, loss = 4.52623132\n",
      "Iteration 72, loss = 4.51434227\n",
      "Iteration 73, loss = 4.49706512\n",
      "Iteration 74, loss = 4.48634321\n",
      "Iteration 75, loss = 4.47177473\n",
      "Iteration 76, loss = 4.46019979\n",
      "Iteration 77, loss = 4.44516611\n",
      "Iteration 78, loss = 4.43382459\n",
      "Iteration 79, loss = 4.42187974\n",
      "Iteration 80, loss = 4.41027372\n",
      "Iteration 81, loss = 4.40023811\n",
      "Iteration 82, loss = 4.38752090\n",
      "Iteration 83, loss = 4.37755220\n",
      "Iteration 84, loss = 4.36466647\n",
      "Iteration 85, loss = 4.35274717\n",
      "Iteration 86, loss = 4.34387233\n",
      "Iteration 87, loss = 4.33297330\n",
      "Iteration 88, loss = 4.32322232\n",
      "Iteration 89, loss = 4.31383074\n",
      "Iteration 90, loss = 4.30124183\n",
      "Iteration 91, loss = 4.29185369\n",
      "Iteration 92, loss = 4.28032831\n",
      "Iteration 93, loss = 4.27351705\n",
      "Iteration 94, loss = 4.26366128\n",
      "Iteration 95, loss = 4.25510744\n",
      "Iteration 96, loss = 4.24969188\n",
      "Iteration 97, loss = 4.23226023\n",
      "Iteration 98, loss = 4.22283950\n",
      "Iteration 99, loss = 4.21612187\n",
      "Iteration 100, loss = 4.20820505\n",
      "Iteration 101, loss = 4.19750361\n",
      "Iteration 102, loss = 4.19424452\n",
      "Iteration 103, loss = 4.18277652\n",
      "Iteration 104, loss = 4.17168429\n",
      "Iteration 105, loss = 4.16491372\n",
      "Iteration 106, loss = 4.15726279\n",
      "Iteration 107, loss = 4.14802107\n",
      "Iteration 108, loss = 4.13989796\n",
      "Iteration 109, loss = 4.13175472\n",
      "Iteration 110, loss = 4.12384251\n",
      "Iteration 111, loss = 4.11680527\n",
      "Iteration 112, loss = 4.10886364\n",
      "Iteration 113, loss = 4.10266463\n",
      "Iteration 114, loss = 4.09163708\n",
      "Iteration 115, loss = 4.08733864\n",
      "Iteration 116, loss = 4.07672851\n",
      "Iteration 117, loss = 4.07139045\n",
      "Iteration 118, loss = 4.06590703\n",
      "Iteration 119, loss = 4.05857941\n",
      "Iteration 120, loss = 4.05089539\n",
      "Iteration 121, loss = 4.04436242\n",
      "Iteration 122, loss = 4.03783010\n",
      "Iteration 123, loss = 4.02876131\n",
      "Iteration 124, loss = 4.02023826\n",
      "Iteration 125, loss = 4.01487453\n",
      "Iteration 126, loss = 4.00752601\n",
      "Iteration 127, loss = 4.00398514\n",
      "Iteration 128, loss = 3.99317999\n",
      "Iteration 129, loss = 3.98738688\n",
      "Iteration 130, loss = 3.98423290\n",
      "Iteration 131, loss = 3.97419735\n",
      "Iteration 132, loss = 3.96557225\n",
      "Iteration 133, loss = 3.96189038\n",
      "Iteration 134, loss = 3.95346658\n",
      "Iteration 135, loss = 3.95393619\n",
      "Iteration 136, loss = 3.94150032\n",
      "Iteration 137, loss = 3.93859549\n",
      "Iteration 138, loss = 3.93548110\n",
      "Iteration 139, loss = 3.92653998\n",
      "Iteration 140, loss = 3.91887334\n",
      "Iteration 141, loss = 3.91617014\n",
      "Iteration 142, loss = 3.90643933\n",
      "Iteration 143, loss = 3.90075632\n",
      "Iteration 144, loss = 3.89534916\n",
      "Iteration 145, loss = 3.89176532\n",
      "Iteration 146, loss = 3.88304871\n",
      "Iteration 147, loss = 3.87959922\n",
      "Iteration 148, loss = 3.87371377\n",
      "Iteration 149, loss = 3.86307305\n",
      "Iteration 150, loss = 3.85921413\n",
      "Iteration 151, loss = 3.85546502\n",
      "Iteration 152, loss = 3.85041329\n",
      "Iteration 153, loss = 3.84895421\n",
      "Iteration 154, loss = 3.84108936\n",
      "Iteration 155, loss = 3.83798362\n",
      "Iteration 156, loss = 3.82877556\n",
      "Iteration 157, loss = 3.82269226\n",
      "Iteration 158, loss = 3.81568878\n",
      "Iteration 159, loss = 3.81445026\n",
      "Iteration 160, loss = 3.80939901\n",
      "Iteration 161, loss = 3.80269899\n",
      "Iteration 162, loss = 3.79919782\n",
      "Iteration 163, loss = 3.79307655\n",
      "Iteration 164, loss = 3.79001714\n",
      "Iteration 165, loss = 3.78564306\n",
      "Iteration 166, loss = 3.77649327\n",
      "Iteration 167, loss = 3.77216591\n",
      "Iteration 168, loss = 3.76555670\n",
      "Iteration 169, loss = 3.76195593\n",
      "Iteration 170, loss = 3.75630958\n",
      "Iteration 171, loss = 3.75688089\n",
      "Iteration 172, loss = 3.74918663\n",
      "Iteration 173, loss = 3.74350705\n",
      "Iteration 174, loss = 3.73725509\n",
      "Iteration 175, loss = 3.73179744\n",
      "Iteration 176, loss = 3.73089694\n",
      "Iteration 177, loss = 3.72374210\n",
      "Iteration 178, loss = 3.72162830\n",
      "Iteration 179, loss = 3.71540594\n",
      "Iteration 180, loss = 3.71302391\n",
      "Iteration 181, loss = 3.70679939\n",
      "Iteration 182, loss = 3.70613874\n",
      "Iteration 183, loss = 3.70326887\n",
      "Iteration 184, loss = 3.69534907\n",
      "Iteration 185, loss = 3.69335735\n",
      "Iteration 186, loss = 3.68457886\n",
      "Iteration 187, loss = 3.68224002\n",
      "Iteration 188, loss = 3.67907400\n",
      "Iteration 189, loss = 3.67622674\n",
      "Iteration 190, loss = 3.66677604\n",
      "Iteration 191, loss = 3.66257989\n",
      "Iteration 192, loss = 3.66412466\n",
      "Iteration 193, loss = 3.65323781\n",
      "Iteration 194, loss = 3.64844157\n",
      "Iteration 195, loss = 3.64974975\n",
      "Iteration 196, loss = 3.64544438\n",
      "Iteration 197, loss = 3.64416312\n",
      "Iteration 198, loss = 3.63399976\n",
      "Iteration 199, loss = 3.63059671\n",
      "Iteration 200, loss = 3.62561037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\Documents\\Code\\KU\\Spring_22\\EECS_738\\Project\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20, 20), verbose=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = MLPClassifier(verbose=True, hidden_layer_sizes=(20, 20, 20))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model.p\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.3931162381341585']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\Documents\\Code\\KU\\Spring_22\\EECS_738\\Project\\.venv\\lib\\site-packages\\sklearn\\base.py:566: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  X = check_array(X, **check_params)\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(Xpred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81a6dc1246209308a911193967922c67d378c3e6826fb6f54760527125705fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
